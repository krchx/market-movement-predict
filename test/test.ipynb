{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e66844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import joblib\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"Load and clean the OHLCV data.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Rename columns to lowercase if they exist with capital letters\n",
    "    column_mapping = {\n",
    "        'Timestamp': 'timestamp',\n",
    "        'Open': 'open',\n",
    "        'High': 'high', \n",
    "        'Low': 'low',\n",
    "        'Close': 'close',\n",
    "        'Volume': 'volume'\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
    "    \n",
    "    # Convert timestamp to datetime if it exists\n",
    "    if 'timestamp' in df.columns:\n",
    "        # Convert Unix timestamp to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        # If not shorted, sort by datetime\n",
    "        # df = df.sort_values('datetime')\n",
    "    \n",
    "    # Drop any duplicates or NaN values\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Calculate all required features from the OHLCV data.\"\"\"\n",
    "\n",
    "    # percentage change for open, high, low, close --> four features\n",
    "    for col in ['open', 'high', 'low', 'close']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_return'] = df[col].pct_change()\n",
    "    \n",
    "    # Volume log-transformation, log(volume + 1) --> one feature\n",
    "    if 'volume' in df.columns:\n",
    "        df['volume_change'] = np.log1p(df['volume'])\n",
    "    \n",
    "    # Volume Velocity (% change in Volume from previous minute) --> one feature\n",
    "    if 'volume' in df.columns:\n",
    "        df['volume_velocity'] = df['volume'].pct_change()\n",
    "    \n",
    "    # Relative Position in Range ((C-L)/(H-L)) --> one feature\n",
    "    if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:\n",
    "        df['relative_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
    "\n",
    "    # Cyclical time features, from 3:45 to 10:00, sin and cos --> two features\n",
    "    minutes_in_day = 10 * 60 - (3 * 60 + 45)\n",
    "    minute_of_day = (df['datetime'].dt.hour - 3) * 60 + (df['datetime'].dt.minute - 45)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * minute_of_day / minutes_in_day)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * minute_of_day / minutes_in_day)\n",
    "\n",
    "    # Cyclical day of year features, sin and cos --> two features\n",
    "    days_in_year = 365\n",
    "    day_of_year = df['datetime'].dt.dayofyear\n",
    "    df['day_sin'] = np.sin(2 * np.pi * day_of_year / days_in_year)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * day_of_year / days_in_year)\n",
    "    \n",
    "    # Drop rows with NaN (first row after pct_change)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_target(df, horizon=5, threshold=0.0015):\n",
    "    \"\"\"\n",
    "    Create target labels for price direction prediction based on future price movement.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'high', 'low', 'close' columns.\n",
    "        horizon (int): Number of future minutes to look ahead.\n",
    "        threshold (float): Percentage threshold for price movement (e.g., 0.001 for 0.1%).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the added 'target' column and rows with NaN targets dropped.\n",
    "    \"\"\"\n",
    "    # Explicitly create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate the max high and min low over the next 'horizon' periods\n",
    "    # Shift(-horizon) looks into the future\n",
    "    future_max_high = df['high'].rolling(window=horizon).max().shift(-horizon)\n",
    "    future_min_low = df['low'].rolling(window=horizon).min().shift(-horizon)\n",
    "\n",
    "    # Calculate the threshold bounds based on the current close\n",
    "    upper_bound = df['close'] * (1 + threshold)\n",
    "    lower_bound = df['close'] * (1 - threshold)\n",
    "\n",
    "    # Define conditions for target labels\n",
    "    conditions = [\n",
    "        future_max_high > upper_bound,  # Price goes up significantly\n",
    "        future_min_low < lower_bound   # Price goes down significantly\n",
    "    ]\n",
    "\n",
    "    # Define choices corresponding to conditions: 2 for up, 0 for down\n",
    "    choices = [2, 0]  # 2 for up, 0 for down\n",
    "\n",
    "    # Apply conditions using np.select, default is 1 (sideways)\n",
    "    # Assign directly to the copied DataFrame\n",
    "    df['target'] = np.select(conditions, choices, default=1)\n",
    "\n",
    "    # Drop last 'horizon' rows where target cannot be calculated\n",
    "    df = df[:-horizon]\n",
    "\n",
    "    # Ensure target is integer type\n",
    "    df['target'] = df['target'].astype(np.int64)\n",
    "\n",
    "    df = df.dropna()  # Drop any rows with NaN values in the target column\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_features(df):\n",
    "    \"\"\"Create two different dataframes: one with input features and other with all features.\"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # Create a df with (timestamp, open, high, low, close, volume)\n",
    "    df_original = df_features[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "    \n",
    "    # Remove columns (timestamp, open, high, low, close, volume, datetime) in df_features\n",
    "    columns_to_remove = ['open', 'high', 'low', 'close', 'volume', 'datetime']\n",
    "    df_features.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df_features, df_original\n",
    "\n",
    "\n",
    "def split_data(df, train_size=0.7, val_size=0.15):\n",
    "    \"\"\"Split data chronologically into train, validation, and test sets.\"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_size)\n",
    "    val_end = int(n * (train_size + val_size))\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "\n",
    "    test_original_indices = df.index[val_end:].tolist()\n",
    "    \n",
    "    return train_df, val_df, test_df, test_original_indices\n",
    "\n",
    "def scale_features(train_df, val_df, test_df):\n",
    "    \"\"\"Scale the features using StandardScaler fit on training data.\"\"\"\n",
    "    # Get numerical feature columns (exclude target and datetime)\n",
    "    feature_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    feature_cols = [col for col in feature_cols if col not in ['target']]\n",
    "    \n",
    "    # Fit scaler on training data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[feature_cols])\n",
    "\n",
    "    # Save the scaler as scaler.joblib\n",
    "    # joblib.dump(scaler, 'data/scaler.joblib')\n",
    "    \n",
    "    # Transform all datasets\n",
    "    train_df[feature_cols] = scaler.transform(train_df[feature_cols])\n",
    "    val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "    test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "    \n",
    "    return train_df, val_df, test_df, feature_cols\n",
    "\n",
    "def create_sequences(df, seq_length=60):\n",
    "    \"\"\"Create sequences for time series model input.\"\"\"\n",
    "    feature_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    feature_cols = [col for col in feature_cols if col not in ['target']]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data_array = df[feature_cols].values\n",
    "    target_array = df['target'].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length + 1):\n",
    "        X.append(data_array[i:i+seq_length])\n",
    "        y.append(target_array[i+seq_length-1])\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoaders for train, validation, and test sets.\"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    \n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def prepare_data(file_path, seq_length=60, horizon=5, batch_size=32):\n",
    "    \"\"\"Full data preparation pipeline.\"\"\"\n",
    "    # Load and clean data\n",
    "    df = load_and_clean_data(file_path)\n",
    "    \n",
    "    # Calculate features\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Create target\n",
    "    df = create_target(df, horizon=horizon)\n",
    "\n",
    "    # Split features for model input\n",
    "    df_features, df_original = split_features(df)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df, test_df, test_original_indices  = split_data(df_features, train_size=0.7, val_size=0.15)\n",
    "\n",
    "    # Ensure the test set is aligned with the original data\n",
    "    if len(test_original_indices) != len(test_df):\n",
    "        print(f\"Warning: Length mismatch between test_original_indices ({len(test_original_indices)}) and test_df ({len(test_df)}).\")\n",
    "    \n",
    "    # Scale features\n",
    "    train_df, val_df, test_df, feature_cols = scale_features(train_df, val_df, test_df)\n",
    "\n",
    "    print(len(train_df), len(val_df), len(test_df))\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_df, seq_length=seq_length)\n",
    "    X_val, y_val = create_sequences(val_df, seq_length=seq_length)\n",
    "    X_test, y_test = create_sequences(test_df, seq_length=seq_length)\n",
    "\n",
    "    print(f\"X_train shape: {X_test.shape}, y_train shape: {y_test.shape}\")\n",
    "    print(len(test_original_indices))\n",
    "\n",
    "    # Align original indices & features with sequences\n",
    "    aligned_test_original_indices = test_original_indices[seq_length-1:]\n",
    "    aligned_test_original_df = df_original.loc[aligned_test_original_indices]\n",
    "\n",
    "    # Ensure the test set is aligned with the original data\n",
    "    if len(y_test) != len(aligned_test_original_df):\n",
    "         print(f\"Warning: Length mismatch between y_test ({len(y_test)}) and aligned_test_original_df ({len(aligned_test_original_df)}). Truncating.\")\n",
    "         min_len = min(len(y_test), len(aligned_test_original_df))\n",
    "         y_test = y_test[:min_len]\n",
    "         X_test = X_test[:min_len]\n",
    "         aligned_test_original_df = aligned_test_original_df[:min_len]\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    input_dim = X_train.shape[2]\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, aligned_test_original_df, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c33c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529 114 114\n",
      "X_train shape: (55, 60, 12), y_train shape: (55,)\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, aligned_test_original_df, input_dim = prepare_data('data.csv')\n",
    "\n",
    "# print(aligned_test_original_df['close'].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
